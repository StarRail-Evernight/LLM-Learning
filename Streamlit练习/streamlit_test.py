import streamlit as st
from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage, AIMessage

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="Qwen3 æœ¬åœ°èŠå¤©åŠ©æ‰‹",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# é¡µé¢æ ‡é¢˜
st.title("ğŸ¤– Qwen3 æœ¬åœ°èŠå¤©åŠ©æ‰‹")
st.markdown("---")

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.header("âš™ï¸ æ¨¡å‹é…ç½®")

    # æ¨¡å‹å‚æ•°è®¾ç½®
    model_name = st.selectbox(
        "é€‰æ‹©æ¨¡å‹",
        options=["qwen3:4b_q4_k_m", "deepseek-r1:8b", "qwen3:8b"],
        index=0,
        help="éœ€è¦åœ¨æœ¬åœ°Ollamaä¸­å·²ä¸‹è½½çš„æ¨¡å‹"
    )
    temperature = st.slider(
        "æ¸©åº¦ç³»æ•°",
        min_value=0.0,
        max_value=1.0,
        value=0.5,
        step=0.1,
        help="æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ï¼Œå€¼è¶Šé«˜è¶Šéšæœºï¼Œè¶Šä½è¶Šç¡®å®š"
    )

    base_url = st.text_input(
        "OllamaæœåŠ¡åœ°å€",
        value="http://localhost:11434",
        help="æœ¬åœ°OllamaæœåŠ¡çš„åœ°å€å’Œç«¯å£"
    )

    st.markdown("---")
    st.info(
        "ğŸ“‹ ä½¿ç”¨è¯´æ˜ï¼š\n"
        "1. ç¡®ä¿æœ¬åœ°OllamaæœåŠ¡å·²å¯åŠ¨ï¼ˆollama serveï¼‰\n"
        "2. å·²ä¸‹è½½å¯¹åº”æ¨¡å‹ï¼ˆollama pull æ¨¡å‹åï¼‰\n"
        "3. åœ¨è¾“å…¥æ¡†ä¸­è¾“å…¥é—®é¢˜å¹¶å‘é€"
    )

# åˆå§‹åŒ–å¯¹è¯å†å²ï¼ˆä½¿ç”¨session stateæŒä¹…åŒ–ï¼‰
if "messages" not in st.session_state:
    st.session_state.messages = []

# åˆå§‹åŒ–æ¨¡å‹ï¼ˆä½¿ç”¨session stateé¿å…é‡å¤åˆ›å»ºï¼‰
@st.cache_resource(show_spinner="æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
def init_model(model, temp, url):
    try:
        return ChatOllama(
            model=model,
            temperature=temp,
            base_url=url,
        )
    except Exception as e:
        st.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼š{str(e)}")
        return None

# åˆå§‹åŒ–æ¨¡å‹
model = init_model(model_name, temperature, base_url)

# æ˜¾ç¤ºå¯¹è¯å†å²
for message in st.session_state.messages:
    if isinstance(message, HumanMessage):
        with st.chat_message("user"):
            st.markdown(message.content)
    elif isinstance(message, AIMessage):
        with st.chat_message("assistant"):
            st.markdown(message.content)

# èŠå¤©è¾“å…¥æ¡†
if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜..."):
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åˆå§‹åŒ–æˆåŠŸ
    if model is None:
        st.error("æ¨¡å‹æœªåˆå§‹åŒ–æˆåŠŸï¼Œè¯·æ£€æŸ¥é…ç½®å’ŒOllamaæœåŠ¡çŠ¶æ€ï¼")
    else:
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å¯¹è¯å†å²
        st.session_state.messages.append(HumanMessage(content=prompt))

        # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        with st.chat_message("user"):
            st.markdown(prompt)

        # ç”ŸæˆåŠ©æ‰‹å›å¤
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""

            try:
                # æµå¼è·å–å›å¤ï¼ˆæ¨¡æ‹Ÿæ‰“å­—æ•ˆæœï¼‰
                response = model.stream([HumanMessage(content=prompt)])

                for chunk in response:
                    if chunk.content:
                        full_response += chunk.content
                        message_placeholder.markdown(full_response + "â–Œ")

                # æ˜¾ç¤ºå®Œæ•´å›å¤
                message_placeholder.markdown(full_response)

                # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å¯¹è¯å†å²
                st.session_state.messages.append(AIMessage(content=full_response))

            except Exception as e:
                error_msg = f"è¯·æ±‚å¤±è´¥ï¼š{str(e)}"
                message_placeholder.markdown(f"âŒ {error_msg}")
                st.error(error_msg)

# æ¸…é™¤å¯¹è¯å†å²æŒ‰é’®ï¼ˆä¿®å¤åˆ—é…ç½®é”™è¯¯ï¼‰
if st.session_state.messages:
    # åªä½¿ç”¨ä¸¤ä¸ªåˆ—ï¼Œæ¯”ä¾‹ä¹‹å’Œä¸º1.0ï¼Œéƒ½æ˜¯æ­£æ•°
    col1, col2 = st.columns([0.9, 0.1])
    with col2:
        if st.button("ğŸ—‘ï¸ æ¸…é™¤å†å²"):
            st.session_state.messages = []
            st.rerun()